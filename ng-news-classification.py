# -*- coding: utf-8 -*-
"""
notebook.ipynb

Automatically generated by Colaboratory.
"""

# Import Dependencies
import pandas as pd
import os
import seaborn as sns
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split

# To import the Transformer Models
from transformers import AutoTokenizer, DataCollatorWithPadding
from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

# to convert to dataset datatype - the transformers library does not work well with pandas
from datasets import Dataset

# load data
df=pd.read_csv(os.path.join('train.csv'),names=['label','Title','Description'])
print(df.head())

# Data statistics
plt.style.use('fivethirtyeight')
plt.figure(figsize=(8,4))
sns.countplot(x=df['label'])
plt.show()

print(df.info())

df['text']=(df['Title']+df['Description'])
df.drop(columns=['Title','Description'],axis=1,inplace=True)
print(df.head())
print(df['text'][1])

# import regular expressions for removing punctuations and other unnecessary symbols
import re

def remove_punctuations(text):
     text=re.sub(r'[\\-]',' ',text)
     text=re.sub(r'[,.?;:\'(){}!|0-9]','',text)
     return text

df['text']=df['text'].apply(remove_punctuations)
# print(df.head())

# natural language toolkit: for removing stopwords
import nltk

# downloading corpus only would work
nltk.download()

from nltk.corpus import stopwords

stopw=stopwords.words('english')
# print(stopw[:10])

def remove_stopwords(text):
    clean_text=[]
    for word in text.split(' '):
        if word not in stopw:
            clean_text.append(word)
    return ' '.join(clean_text)

df['text']=df['text'].apply(remove_stopwords)

# The class labels are marked as 1,2,3,4 but the model needs 0,1,2,3
# so we reduce 1 from all the class labels
df['label']=df['label'].apply(lambda x:x-1)
print(df.head())

# Final Preprocessed text
print(df['text'][1])

# split data into training and testing
from sklearn.model_selection import train_test_split

# the length of dataframe is 120,000. Training on such large data with a large model would take long.
# Choosing a smaller dataset size for training and testing
train_df,test_df=train_test_split(df[['text','label']],train_size=.3,shuffle=True)
test_df=test_df[:10000]

print(train_df.shape,test_df.shape)

# Pre-trained Bert base uncased model from huggingface.co
model_name='bert-base-uncased'
tokenizer=AutoTokenizer.from_pretrained(model_name)

def preprocess_function(examples):
    """
    Tokenize the text to create input and attention data
    
    in -> dataset (columns = text, label)
    out -> tokenized dataset (columns = text, label, input, attention)
    """
    return tokenizer(examples["text"], truncation=True)


def pipeline(dataframe):
    """
    Prepares the dataframe so that it can be given to the transformer model
    
    in -> pandas dataframe
    out -> tokenized dataset (columns = text, label, input, attention)
    """    
    # This step isn't mentioned anywhere but is vital as Transformers library only seems to work with this Dataset data type
    dataset = Dataset.from_pandas(dataframe, preserve_index=False)
    tokenized_ds = dataset.map(preprocess_function, batched=True)
    tokenized_ds = tokenized_ds.remove_columns('text')
    return tokenized_ds

# convert the text to tokens for training and testing data
tokenized_train = pipeline(train_df)
tokenized_test = pipeline(test_df)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# Fine tuning the model with num_labels=4 (the number of classes in ag-news dataset)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4)

training_args = TrainingArguments(
    output_dir="./results",
    save_strategy = 'epoch',
    optim='adamw_torch',
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
)
# Create trainer from Trainer class from transformer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_test,
    tokenizer=tokenizer,
    data_collator=data_collator,
)

# Train the model
trainer.train()

# Model evaluation
tokenized_test = pipeline(test_df)
tokenized_test = tokenized_test.remove_columns('label')
preds = trainer.predict(tokenized_test)

import numpy as np
from sklearn.metrics import classification_report,confusion_matrix
preds_flat = [np.argmax(x) for x in preds[0]]

# Model evaluation report: precision, recall, f1-score, support, accuracy
print(classification_report(test_df['label'], preds_flat))

# plot confusion matrix
plt.figure(figsize=(10,8))
sns.heatmap(
    confusion_matrix(test_df['label'], preds_flat),
    annot=True,
    xticklabels=['World','Sport','Business','Sci/Tech'],
    yticklabels=['World','Sport','Business','Sci/Tech'],
    cmap=plt.cm.magma_r
)
plt.title('Confusion Matrix')
plt.show()

# save model
trainer.save_model('models')

